{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaling(features, number_type, scale_type='minmax'):\n",
    "    if scale_type == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scale_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "    features_scaled = scaler.fit_transform(features[number_type])\n",
    "    features_scaled_df = pd.DataFrame(features_scaled, columns=number_type)\n",
    "    features_nonnum = features.drop(columns=number_type)\n",
    "\n",
    "    features_df = pd.concat([features_scaled_df, features_nonnum], axis=1)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    encoder = LabelEncoder()\n",
    "    labels = encoder.fit_transform(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(features):\n",
    "    encoder = LabelEncoder()\n",
    "    all_columns = features.columns\n",
    "    for col in all_columns:\n",
    "        if features[col].nunique() == 1:\n",
    "            features = features.drop(columns=col)\n",
    "    \n",
    "    object_type = features.select_dtypes(include='object').columns\n",
    "    features[object_type] = features[object_type].astype('category')\n",
    "    object_binary = []\n",
    "\n",
    "    for obj in object_type:\n",
    "        if features[obj].nunique() == 2:\n",
    "            object_binary.append(obj)\n",
    "\n",
    "    for obj in object_binary:\n",
    "        object_type = object_type.drop(obj)\n",
    "\n",
    "    for obj in object_binary:\n",
    "        features[obj] = encoder.fit_transform(features[obj])\n",
    "\n",
    "    features = pd.get_dummies(features).astype(int)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(features, labels, method=1):\n",
    "    if method == 1:\n",
    "        correlation = features.corrwith(labels)\n",
    "        corr_cols = correlation.abs().nlargest(20).keys().to_list()\n",
    "        selected_features = features[corr_cols]\n",
    "\n",
    "    else:\n",
    "        info_gain = SelectKBest(score_func=mutual_info_classif, k=20)\n",
    "        info_gain.fit(features, labels)\n",
    "        top_features = info_gain.get_support(indices=True)\n",
    "        selected_features = features.iloc[:, top_features]\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    col_list = df.columns.to_list()\n",
    "    df_arr = np.array(df)\n",
    "    normalizer = Normalizer(norm='l1').fit(df_arr)\n",
    "    normalizer.transform(df_arr)\n",
    "    df_ = pd.DataFrame(df_arr, columns=col_list)\n",
    "    return df_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataset):\n",
    "    test_features_s = None\n",
    "    test_labels_df = None\n",
    "    if dataset == 1:\n",
    "        df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "        df.replace({' ': np.nan}, inplace=True)\n",
    "        df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "        #df.dropna(subset=['TotalCharges'], inplace=True)\n",
    "        df['TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].mean())\n",
    "        df = df.drop('customerID', axis=1)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "    elif dataset == 2:\n",
    "        df = pd.read_csv('adult/adult.data', header=None)\n",
    "        test_df = pd.read_csv('adult/adult.test', header=None, skiprows=1)\n",
    "\n",
    "        df.columns = df.columns.astype(str)\n",
    "        test_df.columns = test_df.columns.astype(str)\n",
    "\n",
    "        object_type = df.select_dtypes(include='object').columns\n",
    "        for col in object_type:\n",
    "            df[col] = df[col].str.strip()\n",
    "            test_df[col] = test_df[col].str.strip()\n",
    "            \n",
    "        test_df[test_df.columns[-1]] = test_df[test_df.columns[-1]].str.rstrip('.')\n",
    "\n",
    "        df.replace({'?' : np.nan}, inplace=True)\n",
    "        test_df.replace({'?' : np.nan}, inplace=True)\n",
    "\n",
    "        df.drop_duplicates(inplace=True)\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "        for col in test_df.columns:\n",
    "            test_df[col] = test_df[col].fillna(test_df[col].mode()[0])\n",
    "\n",
    "        test_label_col = test_df.columns[-1]\n",
    "        test_features = test_df.drop(test_label_col, axis=1)\n",
    "        test_labels = test_df[test_label_col]\n",
    "        test_labels = encode_labels(test_labels)\n",
    "        number_type = test_features.select_dtypes(include='number').columns\n",
    "\n",
    "        test_features = encode_features(test_features)\n",
    "        test_features = data_scaling(test_features, number_type)\n",
    "\n",
    "    elif dataset == 3:\n",
    "        df_ = pd.read_csv('creditcard.csv')\n",
    "        label = df_.columns[-1]\n",
    "        df_1 = df_[df_[label] == 1]\n",
    "        df_0_sampled = df_[df_[label] == 0].sample(n=20000, random_state=42)\n",
    "        df = pd.concat([df_1, df_0_sampled])\n",
    "\n",
    "    elif dataset == 4:\n",
    "        df = pd.read_csv('A1.csv')\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "        df.columns = df.columns.astype(str)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    label_col = df.columns[-1]\n",
    "    features = df.drop(label_col, axis=1)\n",
    "    labels = df[label_col]\n",
    "\n",
    "    if dataset != 4:\n",
    "        labels = encode_labels(labels)\n",
    "    number_type = features.select_dtypes(include='number').columns\n",
    "    \n",
    "    features = encode_features(features)\n",
    "    features = data_scaling(features, number_type)\n",
    "\n",
    "    label_df = pd.DataFrame(labels, columns=[label_col])\n",
    "    label_series = label_df[label_col]\n",
    "    selected_features = feature_selection(features, label_series)\n",
    "    # selected_features = normalize(selected_features)\n",
    "\n",
    "    if dataset == 2:\n",
    "        cols = selected_features.columns\n",
    "        test_features_s = test_features[cols]\n",
    "        test_labels_df = pd.DataFrame(test_labels, columns=[test_label_col])\n",
    "        \n",
    "    return selected_features, label_df, test_features_s, test_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionImp:\n",
    "    def __init__(self, alpha=0.001, iterations=1000):\n",
    "        self.alpha = alpha\n",
    "        self.iterations = iterations\n",
    "        self.w = None\n",
    "        self.bias = 0\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "    def fit(self, features, labels):\n",
    "        features_arr = np.array(features)\n",
    "        labels_arr = np.array(labels)\n",
    "        labels_arr = labels_arr.reshape(-1, 1)\n",
    "        n = features.shape[1]   #how many columns\n",
    "        self.w = np.zeros((n, 1))\n",
    "        # self.bias = 0\n",
    "        \n",
    "        for i in range(self.iterations):\n",
    "            h = self.sigmoid(np.dot(features_arr, self.w))\n",
    "            dz = labels_arr - h\n",
    "            \n",
    "            gradient = np.dot(features_arr.T, dz)\n",
    "\n",
    "            self.w = self.w + self.alpha*gradient\n",
    "            # self.bias += (self.alpha*np.sum(dz))\n",
    "\n",
    "    def predict(self, features):\n",
    "        features_arr = np.array(features)\n",
    "        probs = self.sigmoid(np.dot(features_arr, self.w))\n",
    "        predictions = []\n",
    "        for p in probs.flatten():\n",
    "            if p >= 0.5:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return predictions\n",
    "    \n",
    "    def prob_predict(self, features):\n",
    "        features_arr = np.array(features)\n",
    "        probs = self.sigmoid(np.dot(features_arr, self.w))\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(X_test, models):\n",
    "    all_predictions = []\n",
    "    # all_prob = []\n",
    "\n",
    "    for i in range(len(models)):\n",
    "        clf = models[i]\n",
    "        y_pred = clf.predict(X_test)\n",
    "        # y_prob = clf.prob_predict(X_test)\n",
    "        y_pred_df = pd.DataFrame(y_pred)\n",
    "        # y_prob_df = pd.DataFrame(y_prob)\n",
    "        all_predictions.append(y_pred_df)\n",
    "        # all_prob.append(y_prob_df)\n",
    "\n",
    "    votes = pd.concat(all_predictions, axis='columns')\n",
    "    # probs = pd.concat(all_prob, axis='columns')\n",
    "    predictions = votes.mode(axis='columns').to_numpy()\n",
    "    # probabilities = probs.mean(axis='columns').to_numpy()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stacking:\n",
    "    def __init__(self, models, meta):\n",
    "        self.models = models\n",
    "        self.meta = meta\n",
    "\n",
    "    def fit(self, features, labels): #use validation set here\n",
    "        meta_features = []\n",
    "\n",
    "        for model in self.models:\n",
    "            pred = model.predict(features)\n",
    "            meta_features.append(pred)\n",
    "\n",
    "        meta_features = np.array(meta_features).T\n",
    "        meta_df = pd.DataFrame(meta_features)\n",
    "        fpred_df = pd.concat([features, meta_df], axis=1)\n",
    "        #concat meta_features with validation set\n",
    "        fpred_df.columns = fpred_df.columns.astype(str)\n",
    "\n",
    "        self.meta.fit(fpred_df, labels)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        meta_features = [model.predict(X_test) for model in self.models]\n",
    "        meta_features = np.array(meta_features)\n",
    "        meta_features = meta_features.T\n",
    "        meta_df = pd.DataFrame(meta_features)\n",
    "\n",
    "        fpred_df = pd.concat([X_test.reset_index(drop=True), meta_df.reset_index(drop=True)], axis=1)\n",
    "        fpred_df.columns = fpred_df.columns.astype(str)\n",
    "        #append test with meta featues\n",
    "        \n",
    "        y_pred = self.meta.predict(fpred_df)\n",
    "        # y_prob = self.meta.prob_predict(fpred_df)\n",
    "        return y_pred\n",
    "    \n",
    "    # def prob_predict(self, features):\n",
    "    #     return self.meta.prob_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(features, labels):\n",
    "    label_col = labels.columns[0]\n",
    "    bagged_sets = []\n",
    "    n = len(features)\n",
    "    print(n)\n",
    "    print(features.shape)\n",
    "    \n",
    "    training_set = pd.concat([features, labels], axis=1)\n",
    "    for i in range(9):\n",
    "        sample = resample(training_set, n_samples=n, replace=True)\n",
    "        bagged_sets.append(sample)\n",
    "\n",
    "    models = []\n",
    "    for i in range(9):\n",
    "        bag = bagged_sets[i]\n",
    "        bag_x = bag.drop(label_col, axis=1)\n",
    "        bag_y = bag[label_col].to_list()\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(bag_x, bag_y)\n",
    "        models.append(clf)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    # auroc = roc_auc_score(y_test, y_pred_prob)\n",
    "    # aupr = average_precision_score(y_test, y_pred_prob)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(tn, fp, fn, tp)\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Sensitivity': recall,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'F1-score': f1\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920\n",
      "(1920, 2)\n",
      "20 308 0 272\n",
      "32 296 0 272\n",
      "32 296 0 272\n",
      "32 296 0 272\n",
      "32 296 0 272\n",
      "296 32 272 0\n",
      "32 296 0 272\n",
      "12 316 0 272\n",
      "32 296 0 272\n",
      "LR avg +- stdev\n",
      "Accuracy: 0.49925925925925924 +- 0.011523221619336346\n",
      "Sensitivity: 0.8888888888888888 +- 0.3142696805273545\n",
      "Specificity: 0.1761517615176152 +- 0.257616550262406\n",
      "Precision: 0.42275444309718924 +- 0.14956944664240762\n",
      "F1-score: 0.5729744530923699 +- 0.2026420276169207\n",
      "76 252 0 272\n",
      "Stacking Ensemble\n",
      "Accuracy : 0.58\n",
      "Sensitivity : 1.0\n",
      "Specificity : 0.23170731707317074\n",
      "Precision : 0.5190839694656488\n",
      "F1-score : 0.6834170854271356\n"
     ]
    }
   ],
   "source": [
    "dataset = int(input(\"Enter the dataset\"))\n",
    "features, labels, test_features, test_labels = data_preprocessing(dataset)\n",
    "\n",
    "label_col = labels.columns[0]\n",
    "labels_arr = labels[label_col].to_list()\n",
    "\n",
    "\n",
    "if dataset == 1 or dataset == 3 or dataset == 4:\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(features, labels_arr, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=16)\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    X_val.reset_index(drop=True, inplace=True)\n",
    "    y_train_df = pd.DataFrame(y_train, columns=[label_col])\n",
    "\n",
    "elif dataset == 2:\n",
    "    test_labels_arr = test_labels[label_col].to_list()\n",
    "    test_labels_col = test_labels.columns[0]\n",
    "    y_test = test_labels[test_labels_col].to_list()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(features, labels_arr, train_size=0.2, random_state=42)\n",
    "    X_test = test_features\n",
    "    y_test = test_labels_arr\n",
    "    y_train_df = pd.DataFrame(y_train, columns=[label_col])\n",
    "    X_train.reset_index(drop=True, inplace=True)\n",
    "    X_test.reset_index(drop=True, inplace=True)\n",
    "    X_val.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# clf = LogisticRegressionImp()\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# y_prob = clf.prob_predict(X_test)\n",
    "# metrics = performance(y_test, y_pred, y_prob)\n",
    "# for key, val in metrics.items():\n",
    "#     print(f\"{key} : {val}\")\n",
    "\n",
    "\n",
    "models = bagging(X_train, y_train_df)\n",
    "all_metrics = {\n",
    "        'Accuracy': [],\n",
    "        'Sensitivity': [],\n",
    "        'Specificity': [],\n",
    "        'Precision': [],\n",
    "        'F1-score': []\n",
    "    }\n",
    "\n",
    "for model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    # y_prob = model.prob_predict(X_test)\n",
    "    metrics = performance(y_test, y_pred)\n",
    "    for key, val in metrics.items():\n",
    "        all_metrics[key].append(val)\n",
    "\n",
    "print(\"LR avg +- stdev\")\n",
    "for metric, vals in all_metrics.items():\n",
    "    avg = np.mean(vals)\n",
    "    stdev = np.std(vals)\n",
    "    print(f\"{metric}: {avg} +- {stdev}\")\n",
    "\n",
    "# met_df = pd.DataFrame(all_metrics)\n",
    "# df_melted = met_df.melt(var_name='Metric', value_name='Score')\n",
    "\n",
    "# # plt.figure(figsize=(10, 6))\n",
    "# # sns.violinplot(x='Metric', y='Score', data=df_melted, inner=\"point\", palette=\"Set2\", hue='Metric', legend=False)\n",
    "# # plt.title(\"Violin Plot of Performance Metrics\", fontsize=16)\n",
    "# # plt.xlabel(\"Metric\", fontsize=12)\n",
    "# # plt.ylabel(\"Score\", fontsize=12)\n",
    "# # plt.xticks(rotation=45)\n",
    "# # plt.tight_layout()\n",
    "# # plt.show()\n",
    "\n",
    "# y_pred, y_prob = majority_voting(X_test, models)\n",
    "# metrics = performance(y_test, y_pred, y_prob)\n",
    "# print(\"Voting Ensemble\")\n",
    "# for key, val in metrics.items():\n",
    "#     print(f\"{key} : {val}\")\n",
    "# print('\\n')\n",
    "\n",
    "meta = LogisticRegression()\n",
    "stack = stacking(models, meta)\n",
    "stack.fit(X_val, y_val)\n",
    "y_pred= stack.predict(X_test)\n",
    "metrics = performance(y_test, y_pred)\n",
    "print(\"Stacking Ensemble\")\n",
    "for key, val in metrics.items():\n",
    "    print(f\"{key} : {val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
